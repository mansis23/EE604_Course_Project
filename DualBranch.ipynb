{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lrMRqL1J-nP-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, regularizers, Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import zipfile\n",
        "from sklearn.utils import class_weight\n",
        "from skimage.feature import local_binary_pattern"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for the zip file and its extraction location\n",
        "zip_path = '/content/faces_224.zip'\n",
        "unzip_path = '/content'\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_path)\n",
        "# Update the image path\n",
        "image_path = unzip_path"
      ],
      "metadata": {
        "id": "TUOyDHub-_9Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_efficientnet_model(input_shape=(224, 224, 3), feature_shape=(262,)):\n",
        "    \"\"\"\n",
        "    Create model with properly separated image and feature inputs\n",
        "    \"\"\"\n",
        "    # Image input branch\n",
        "    image_input = layers.Input(shape=input_shape, name=\"image_input\")\n",
        "\n",
        "    # Create EfficientNetB0 as a separate model first\n",
        "    efficient_net = EfficientNetB0(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    efficient_net.trainable = False\n",
        "\n",
        "    # Process image through EfficientNet\n",
        "    x = efficient_net(image_input)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Handcrafted feature input branch (separate from EfficientNet)\n",
        "    feature_input = layers.Input(shape=feature_shape, name=\"feature_input\")\n",
        "\n",
        "    # Process handcrafted features\n",
        "    y = layers.BatchNormalization()(feature_input)\n",
        "    y = layers.Dense(256, activation='relu')(y)\n",
        "    y = layers.Dropout(0.3)(y)\n",
        "    y = layers.Dense(128, activation='relu')(y)\n",
        "    y = layers.Dropout(0.2)(y)\n",
        "\n",
        "    # Combine features\n",
        "    combined = layers.Concatenate()([x, y])\n",
        "\n",
        "    # Final classification layers\n",
        "    z = layers.Dense(256, activation='relu')(combined)\n",
        "    z = layers.BatchNormalization()(z)\n",
        "    z = layers.Dropout(0.5)(z)\n",
        "    z = layers.Dense(128, activation='relu')(z)\n",
        "    z = layers.BatchNormalization()(z)\n",
        "    z = layers.Dropout(0.3)(z)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "    # Create model with both inputs\n",
        "    model = Model(\n",
        "        inputs={\n",
        "            \"image_input\": image_input,\n",
        "            \"feature_input\": feature_input\n",
        "        },\n",
        "        outputs=outputs\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "H_lqO1KR_Hs_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFakeDataGenerator:\n",
        "    def __init__(self, dataframe, image_path, batch_size=32, target_size=(224, 224)):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_path = image_path\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.n = len(dataframe)\n",
        "        self.indexes = np.arange(self.n)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.n / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __call__(self):\n",
        "        while True:\n",
        "            for i in range(0, self.n, self.batch_size):\n",
        "                batch_indexes = self.indexes[i:min(i + self.batch_size, self.n)]\n",
        "                batch_data = self.dataframe.iloc[batch_indexes]\n",
        "\n",
        "                images = []\n",
        "                features = []\n",
        "                labels = []\n",
        "\n",
        "                for _, row in batch_data.iterrows():\n",
        "                    img_path = os.path.join(self.image_path, row['videoname'][:-4] + '.jpg')\n",
        "                    try:\n",
        "                        image = cv2.imread(img_path)\n",
        "                        if image is None:\n",
        "                            continue\n",
        "\n",
        "                        image = cv2.resize(image, self.target_size)\n",
        "                        handcrafted_features = extract_features(image)\n",
        "                        image_normalized = image.astype('float32') / 255.0\n",
        "\n",
        "                        images.append(image_normalized)\n",
        "                        features.append(handcrafted_features)\n",
        "                        labels.append(1 if row['label'] == 'FAKE' else 0)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {img_path}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if not images:\n",
        "                    continue\n",
        "\n",
        "                images = np.array(images, dtype=np.float32)\n",
        "                features = np.array(features, dtype=np.float32)\n",
        "                labels = np.array(labels)\n",
        "\n",
        "                yield {\"image_input\": images, \"feature_input\": features}, labels"
      ],
      "metadata": {
        "id": "KmZZbY_D_Js3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_gen, val_gen, steps_per_epoch, validation_steps, epochs=20):\n",
        "    \"\"\"\n",
        "    Create and train the model\n",
        "    \"\"\"\n",
        "    model = create_efficientnet_model(input_shape=(224, 224, 3), feature_shape=(262,))\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=3,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose =1\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def fine_tune_model(model, train_gen, val_gen, steps_per_epoch, validation_steps, epochs=10):\n",
        "    \"\"\"\n",
        "    Fine-tune the model\n",
        "    \"\"\"\n",
        "    # Find the EfficientNet model\n",
        "    efficient_net = None\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.Model):\n",
        "            efficient_net = layer\n",
        "            break\n",
        "\n",
        "    if efficient_net is not None:\n",
        "        efficient_net.trainable = True\n",
        "\n",
        "        # Freeze first 70% of the layers\n",
        "        num_layers = len(efficient_net.layers)\n",
        "        num_to_freeze = int(0.7 * num_layers)\n",
        "        for layer in efficient_net.layers[:num_to_freeze]:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # Recompile with lower learning rate\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Fine-tune\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=epochs,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=3,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "        ],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "_PEAI31GvmCm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image):\n",
        "    \"\"\"\n",
        "    Extract image features with exactly 262 dimensions\n",
        "    \"\"\"\n",
        "    if image.dtype == np.float32 and image.max() <= 1.0:\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "    elif image.dtype != np.uint8:\n",
        "        image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # Color histogram (216 features)\n",
        "    hist = cv2.calcHist([image], [0, 1, 2], None, [6, 6, 6], [0, 256, 0, 256, 0, 256])\n",
        "    color_hist = cv2.normalize(hist, hist).flatten()\n",
        "\n",
        "    # Edge features (16 features)\n",
        "    if len(image.shape) == 3:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray = image\n",
        "    gray = gray.astype(np.uint8)\n",
        "\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    edges = cv2.Canny(blurred, 100, 200)\n",
        "    edge_hist = cv2.calcHist([edges], [0], None, [16], [0, 256])\n",
        "    edge_hist = cv2.normalize(edge_hist, edge_hist).flatten()\n",
        "\n",
        "    # LBP features (10 features)\n",
        "    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n",
        "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
        "    lbp_hist = lbp_hist.astype(np.float32)\n",
        "    lbp_hist = lbp_hist / (lbp_hist.sum() + 1e-7)\n",
        "\n",
        "    # Statistical features (20 features)\n",
        "    stat_features = []\n",
        "    # Global statistics\n",
        "    stat_features.extend([\n",
        "        np.mean(gray),\n",
        "        np.std(gray),\n",
        "        np.median(gray),\n",
        "        np.percentile(gray, 25),\n",
        "        np.percentile(gray, 75),\n",
        "    ])\n",
        "\n",
        "    # Region-based statistics (3 regions: top, middle, bottom)\n",
        "    h = gray.shape[0]\n",
        "    regions = [\n",
        "        gray[:h//3],           # top\n",
        "        gray[h//3:2*h//3],     # middle\n",
        "        gray[2*h//3:]          # bottom\n",
        "    ]\n",
        "\n",
        "    for region in regions:\n",
        "        stat_features.extend([\n",
        "            np.mean(region),\n",
        "            np.std(region),\n",
        "            np.median(region),\n",
        "            np.percentile(region, 25),\n",
        "            np.percentile(region, 75),\n",
        "        ])\n",
        "\n",
        "    stat_features = np.array(stat_features, dtype=np.float32)\n",
        "\n",
        "    # Total features: 216 (color) + 16 (edge) + 10 (LBP) + 20 (statistical) = 262 features\n",
        "    features = np.concatenate([color_hist, edge_hist, lbp_hist, stat_features])\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "ZgCBVhV6BQ9t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load metadata\n",
        "    metadata_path = \"/content/metadata.csv\"\n",
        "    df = pd.read_csv(metadata_path)\n",
        "\n",
        "    # Sample and split data\n",
        "    real_df = df[df[\"label\"] == \"REAL\"]\n",
        "    fake_df = df[df[\"label\"] == \"FAKE\"]\n",
        "    sample_size = 10000\n",
        "    real_df = real_df.sample(sample_size, random_state=42)\n",
        "    fake_df = fake_df.sample(sample_size, random_state=42)\n",
        "    sample_meta = pd.concat([real_df, fake_df])\n",
        "\n",
        "    # Split into train, validation, and test sets\n",
        "    Train_set, Test_set = train_test_split(sample_meta, test_size=0.2, random_state=42, stratify=sample_meta['label'])\n",
        "    Train_set, Val_set = train_test_split(Train_set, test_size=0.3, random_state=42, stratify=Train_set['label'])\n",
        "\n",
        "    # Create data generators\n",
        "    batch_size = 32\n",
        "    train_generator = DeepFakeDataGenerator(Train_set, image_path, batch_size)\n",
        "    val_generator = DeepFakeDataGenerator(Val_set, image_path, batch_size)\n",
        "    test_generator = DeepFakeDataGenerator(Test_set, image_path, batch_size)\n",
        "\n",
        "    # Calculate steps per epoch\n",
        "    steps_per_epoch = len(train_generator)\n",
        "    validation_steps = len(val_generator)\n",
        "\n",
        "    # Create and train the initial model\n",
        "    model = create_efficientnet_model(input_shape=(224, 224, 3), feature_shape=(288,))\n",
        "\n",
        "    # Initial training\n",
        "    history = train_model(\n",
        "        train_generator(),\n",
        "        val_generator(),\n",
        "        steps_per_epoch,\n",
        "        validation_steps,\n",
        "        epochs=20,\n",
        "    )\n",
        "\n",
        "    # Fine-tuning\n",
        "    # Unfreeze the EfficientNet layers\n",
        "    base_model = model.get_layer('efficientnetb0')\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Recompile with a lower learning rate\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    history_ft = model.fit(\n",
        "        train_generator(),\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_generator(),\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=10,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_accuracy = model.evaluate(\n",
        "        test_generator(),\n",
        "        steps=len(test_generator)\n",
        "    )\n",
        "    print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save('deepfake_detector_efficientnetb0.h5')\n",
        "\n",
        "    # If running in Google Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download('deepfake_detector_efficientnetb0.h5')\n",
        "    except ImportError:\n",
        "        print(\"Model saved locally as 'deepfake_detector_efficientnetb0.h5'\")\n",
        "\n",
        "    # Create visualization of training history\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    # Initial training\n",
        "    plt.plot(history[0].history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history[0].history['val_accuracy'], label='Validation Accuracy')\n",
        "    # Fine-tuning\n",
        "    if len(history_ft.history['accuracy']) > 0:\n",
        "        offset = len(history[0].history['accuracy'])\n",
        "        epochs_ft = np.arange(offset, offset + len(history_ft.history['accuracy']))\n",
        "        plt.plot(epochs_ft, history_ft.history['accuracy'],\n",
        "                label='Fine-tuning Training Accuracy', linestyle='--')\n",
        "        plt.plot(epochs_ft, history_ft.history['val_accuracy'],\n",
        "                label='Fine-tuning Validation Accuracy', linestyle='--')\n",
        "\n",
        "    plt.title('Model Accuracy Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Initial training\n",
        "    plt.plot(history[0].history['loss'], label='Training Loss')\n",
        "    plt.plot(history[0].history['val_loss'], label='Validation Loss')\n",
        "    # Fine-tuning\n",
        "    if len(history_ft.history['loss']) > 0:\n",
        "        offset = len(history[0].history['loss'])\n",
        "        epochs_ft = np.arange(offset, offset + len(history_ft.history['loss']))\n",
        "        plt.plot(epochs_ft, history_ft.history['loss'],\n",
        "                label='Fine-tuning Training Loss', linestyle='--')\n",
        "        plt.plot(epochs_ft, history_ft.history['val_loss'],\n",
        "                label='Fine-tuning Validation Loss', linestyle='--')\n",
        "\n",
        "    plt.title('Model Loss Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print final metrics summary\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(f\"Best validation accuracy: {max(history[0].history['val_accuracy']):.4f}\")\n",
        "    print(f\"Best validation loss: {min(history[0].history['val_loss']):.4f}\")\n",
        "    if len(history_ft.history['val_accuracy']) > 0:\n",
        "        print(f\"Best fine-tuning validation accuracy: {max(history_ft.history['val_accuracy']):.4f}\")\n",
        "        print(f\"Best fine-tuning validation loss: {min(history_ft.history['val_loss']):.4f}\")\n",
        "    print(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Final test loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "2H4YQlvo_Ocn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}